[{"title":"微积分入门","slug":"article (3)","content":" 导数与积分 基本导数公式：$\\\\frac{d}{dx}x^n = nx^{n 1}$ 积分公式：$\\\\int x^n dx = \\\\frac{x^{n+1}}{n+1} + C$ ","tags":["数学","微积分"],"author":"你的名字","date":"2025-07-26T00:00:00.000Z"},{"title":"线性代数基础","slug":"post (1)","content":" 矩阵运算 矩阵乘法公式：$AB = \\\\sum {k=1}^n A {ik}B {kj}$ 特征值方程：$A\\\\mathbf{v} = \\\\lambda\\\\mathbf{v}$ 行列式计算：$\\\\det A = \\\\sum {\\\\sigma \\\\in S n} \\\\text{sgn} \\\\sigma \\\\prod {i=1}^n a {i,\\\\sigma i }$ ","tags":["数学","线性代数"],"author":"你的名字","date":"2025-07-25T00:00:00.000Z"},{"title":"线性代数基础","slug":"article (4)","content":" 矩阵运算 矩阵乘法公式：$AB = \\\\sum {k=1}^n A {ik}B {kj}$ 特征值方程：$A\\\\mathbf{v} = \\\\lambda\\\\mathbf{v}$ ","tags":["数学","线性代数"],"author":"你的名字","date":"2025-07-25T00:00:00.000Z"},{"title":"概率论基础","slug":"article (2)","content":" 概率公式 贝叶斯定理：$P A|B = \\\\frac{P B|A P A }{P B }$ 期望值：$E X = \\\\sum x p x $ ","tags":["数学","概率"],"author":"你的名字","date":"2025-07-22T00:00:00.000Z"},{"title":"机器学习中的数学","slug":"post (2)","content":" 核心概念 梯度下降：$\\\\theta {t+1} = \\\\theta t \\\\eta \\\\nabla J \\\\theta t $ 逻辑回归：$h \\\\theta x = \\\\frac{1}{1+e^{ \\\\theta^T x}}$ 核方法：$k x,x\\' = \\\\phi x ^T \\\\phi x\\' $ ","tags":["机器学习","数学"],"author":"你的名字","date":"2025-07-13T00:00:00.000Z"},{"title":"机器学习学","slug":"post (2) copy","content":" 核心概念 梯度下降：$\\\\theta {t+1} = \\\\theta t \\\\eta \\\\nabla J \\\\theta t $ 逻辑回归：$h \\\\theta x = \\\\frac{1}{1+e^{ \\\\theta^T x}}$ 核方法：$k x,x\\' = \\\\phi x ^T \\\\phi x\\' $ ","tags":["机器学习","数学"],"author":"你的名字","date":"2025-07-12T00:00:00.000Z"},{"title":"机器学习中的数学公式测试","slug":"article (1)","content":" 基础符号与上下标 变量与下标：$x i, y j, z {k+1}$ 上标与幂运算：$a^2, b^n, e^{kx}, 2^{i+j}$ 混合上下标：$x^m n, \\\\theta^{t+1} i, \\\\alpha^{ \\\\ell } {j}$ 希腊字母：$\\\\alpha, \\\\beta, \\\\gamma, \\\\delta, \\\\epsilon, \\\\lambda, \\\\mu, \\\\sigma, \\\\omega$ 大写希腊字母：$\\\\Lambda, \\\\Sigma, \\\\Omega, \\\\Delta$ 线性代数 向量：$\\\\mathbf{x}, \\\\mathbf{y}, \\\\mathbf{w} = w 1, w 2, ..., w n ^T$ 矩阵：$\\\\mathbf{A}, \\\\mathbf{B}, \\\\mathbf{X} \\\\in \\\\mathbb{R}^{m \\\\times n}$ 矩阵乘法：$\\\\mathbf{C} = \\\\mathbf{A} \\\\mathbf{B}$，其中 $C {ij} = \\\\sum {k=1}^n A {ik} B {kj}$ 转置：$\\\\mathbf{A}^T, \\\\mathbf{x} \\\\mathbf{y}^T ^T$ 逆矩阵：$\\\\mathbf{A}^{ 1}, \\\\mathbf{B}^T \\\\mathbf{B} ^{ 1}$ 特征值与特征向量：$\\\\mathbf{A} \\\\mathbf{v} = \\\\lambda \\\\mathbf{v}$ 微积分 导数：$\\\\frac{dy}{dx}, \\\\frac{\\\\partial f}{\\\\partial x i}, \\\\nabla f x $ 偏导数：$\\\\frac{\\\\partial L}{\\\\partial \\\\theta}, \\\\frac{\\\\partial^2 J}{\\\\partial w \\\\partial b}$ 梯度：$\\\\nabla J \\\\theta = \\\\left \\\\frac{\\\\partial J}{\\\\partial \\\\theta 1}, \\\\frac{\\\\partial J}{\\\\partial \\\\theta 2}, ..., \\\\frac{\\\\partial J}{\\\\partial \\\\theta n} \\\\right ^T$ 积分：$\\\\int a^b f x dx, \\\\iint D g x,y dxdy, \\\\int { \\\\infty}^\\\\infty e^{ x^2} dx = \\\\sqrt{\\\\pi}$ 机器学习核心公式 线性回归：$h \\\\theta x = \\\\theta 0 + \\\\theta 1 x 1 + ... + \\\\theta n x n = \\\\theta^T \\\\mathbf{x}$ 逻辑回归：$h \\\\theta x = \\\\frac{1}{1 + e^{ \\\\theta^T \\\\mathbf{x}}}$ 损失函数：$L y, \\\\hat{y} = y \\\\log \\\\hat{y} + 1 y \\\\log 1 \\\\hat{y} $ 梯度下降更新：$\\\\theta j = \\\\theta j \\\\eta \\\\frac{\\\\partial J \\\\theta }{\\\\partial \\\\theta j}$ softmax 函数：$\\\\sigma \\\\mathbf{z} i = \\\\frac{e^{z i}}{\\\\sum {k=1}^K e^{z k}}$ 卷积操作：$ f g x = \\\\int { \\\\infty}^\\\\infty f t g x t dt$ 概率与统计 概率分布：$P X=x , p y | x; \\\\theta , \\\\mathcal{N} \\\\mu, \\\\sigma^2 $ 期望与方差：$\\\\mathbb{E} X , \\\\text{Var} X = \\\\mathbb{E} X^2 \\\\mathbb{E} X ^2$ 贝叶斯公式：$P A|B = \\\\frac{P B|A P A }{P B }$ 复杂公式块 $$ \\\\min {\\\\theta} J \\\\theta = \\\\frac{1}{m} \\\\sum {i=1}^m L h \\\\theta x^{ i } , y^{ i } + \\\\lambda \\\\sum {j=1}^n \\\\theta j^2 $$ $$ \\\\mathbf{W}^ = \\\\arg \\\\min {\\\\mathbf{W}} \\\\left\\\\| \\\\mathbf{X} \\\\mathbf{W} \\\\mathbf{H} \\\\right\\\\| F^2 + \\\\lambda \\\\|\\\\mathbf{W}\\\\| 1 $$ $$ \\\\begin{bmatrix} a {11} & a {12} & \\\\dots & a {1n} \\\\\\\\ a {21} & a {22} & \\\\dots & a {2n} \\\\\\\\ \\\\vdots & \\\\vdots & \\\\ddots & \\\\vdots \\\\\\\\ a {m1} & a {m2} & \\\\dots & a {mn} \\\\end{bmatrix} \\\\begin{bmatrix} x 1 \\\\\\\\ x 2 \\\\\\\\ \\\\vdots \\\\\\\\ x n \\\\end{bmatrix} = \\\\begin{bmatrix} b 1 \\\\\\\\ b 2 \\\\\\\\ \\\\vdots \\\\\\\\ b m \\\\end{bmatrix} $$ ","tags":["机器学习","数学","公式测试"],"author":"测试者","date":"2025-03-13T00:00:00.000Z"}]